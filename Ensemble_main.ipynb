{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1acae0d3-a407-40ee-b29b-572bd3741ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 129\u001b[0m\n\u001b[0;32m    127\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhalf-closed\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 129\u001b[0m     face_preprocessed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m(face)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# Predict the label using ensembled models\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     preds_vgg16 \u001b[38;5;241m=\u001b[39m model_vgg16\u001b[38;5;241m.\u001b[39mpredict(face_preprocessed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocess_image' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load your trained models\n",
    "model_vgg16 = load_model(\"C:\\\\Projects\\\\Python\\\\ELC\\\\VGG16_New.keras\")\n",
    "model_vgg19 = load_model(\"C:\\\\Projects\\\\Python\\\\ELC\\\\VGG19_New.keras\")\n",
    "model_DenseNet = load_model(\"C:\\\\Projects\\\\Python\\\\ELC\\\\DenseNet121_New.keras\")\n",
    "\n",
    "# Define labels for facial expression detection\n",
    "labels = ['closed eyes', 'drowsy', 'half-closed', 'neutral', 'yawning']\n",
    "\n",
    "# Function to preprocess frames for VGG models\n",
    "def preprocess_frame(frame, target_size=(224, 224)):\n",
    "    frame = cv2.resize(frame, target_size)\n",
    "    frame = frame.astype('float32') / 255.0\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "    return frame\n",
    "\n",
    "# Function to perform ensemble prediction\n",
    "def ensemble_predictions(preds_vgg16, preds_vgg19, preds_DenseNet):\n",
    "    preds = np.array([preds_vgg16, preds_vgg19, preds_DenseNet])\n",
    "    summed_preds = np.sum(preds, axis=0)\n",
    "    averaged_preds = summed_preds / 3\n",
    "    return averaged_preds, np.argmax(averaged_preds)\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to calculate Mouth Aspect Ratio (MAR)\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = np.linalg.norm(mouth[2] - mouth[10])  # upper_lip_top - lower_lip_bottom\n",
    "    B = np.linalg.norm(mouth[4] - mouth[8])   # upper_lip_bottom - lower_lip_top\n",
    "    C = np.linalg.norm(mouth[0] - mouth[6])   # left_corner - right_corner\n",
    "    mar = (A + B) / (2.0 * C)\n",
    "    return mar\n",
    "\n",
    "# Function to calculate head tilt using nose landmarks\n",
    "def calculate_head_tilt(nose_tip, chin):\n",
    "    delta_x = chin[0] - nose_tip[0]\n",
    "    delta_y = chin[1] - nose_tip[1]\n",
    "    angle = np.arctan2(delta_y, delta_x) * 180 / np.pi\n",
    "    return angle\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Frame rate (frames per second)\n",
    "frame_rate = 30\n",
    "\n",
    "# Calculate the interval between frames in milliseconds\n",
    "frame_interval = int(1000 / frame_rate)\n",
    "\n",
    "# Threshold for mouth aspect ratio to determine if mouth is open\n",
    "mouth_open_threshold = 0.20  # Adjust as needed\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces using Haar Cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "        # Detect landmarks using face_recognition\n",
    "        face_landmarks_list = face_recognition.face_landmarks(face)\n",
    "        if not face_landmarks_list:\n",
    "            continue\n",
    "        \n",
    "        face_landmarks = face_landmarks_list[0]\n",
    "        \n",
    "        # Get the landmarks for the eyes, mouth, nose, and chin\n",
    "        left_eye = np.array(face_landmarks['left_eye'])\n",
    "        right_eye = np.array(face_landmarks['right_eye'])\n",
    "        top_lip = np.array(face_landmarks['top_lip'])\n",
    "        bottom_lip = np.array(face_landmarks['bottom_lip'])\n",
    "        nose_tip = np.array(face_landmarks['nose_tip'][2])\n",
    "        chin = np.array(face_landmarks['chin'][8])\n",
    "        mouth = np.concatenate((top_lip, bottom_lip), axis=0)\n",
    "        \n",
    "        # Calculate EAR\n",
    "        ear_left = eye_aspect_ratio(left_eye)\n",
    "        ear_right = eye_aspect_ratio(right_eye)\n",
    "        \n",
    "        # Calculate MAR\n",
    "        mar = mouth_aspect_ratio(mouth)\n",
    "        \n",
    "        # Calculate head tilt\n",
    "        head_tilt_angle = calculate_head_tilt(nose_tip, chin)\n",
    "        \n",
    " # Determine the label based on EAR, MAR, and head tilt\n",
    "        if mar > mouth_open_threshold:\n",
    "            label = 'yawning'\n",
    "        elif ear_left > 0.23 and ear_right > 0.23:\n",
    "            label = 'neutral'\n",
    "        # elif ear_left < 0.20 and ear_right < 0.20 :\n",
    "        #     label = 'CLOSED_EYES'\n",
    "        elif ear_left < 0.20 and ear_right < 0.20 and head_tilt_angle > 15:  # Adjust the threshold for head tilt as needed\n",
    "            label = 'drowsy'            \n",
    "        elif 0.20 <= ear_left < 0.23 or 0.2 <= ear_right < 0.23:\n",
    "            label = 'half-closed'\n",
    "        else:\n",
    "            face_preprocessed = preprocess_image(face)\n",
    "            \n",
    "            # Predict the label using ensembled models\n",
    "            preds_vgg16 = model_vgg16.predict(face_preprocessed)\n",
    "            preds_vgg19 = model_vgg19.predict(face_preprocessed)\n",
    "            preds_DenseNet = model_DenseNet.predict(face_preprocessed)\n",
    "            \n",
    "            averaged_preds, final_prediction = ensemble_predictions(preds_vgg16, preds_vgg19, preds_DenseNet)\n",
    "            \n",
    "            # Check if final_prediction index is within valid range\n",
    "            if final_prediction < len(labels):\n",
    "                label = labels[final_prediction]\n",
    "            else:\n",
    "                label = \"Unknown\"\n",
    "        \n",
    "        # Display the label on the frame\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(frame_interval) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all windows\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2946a4a3-1401-404f-83fb-6cfdc40b643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "from keras.models import load_model\n",
    "\n",
    "# Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Load your trained models\n",
    "model_vgg16 = load_model(\"C:\\\\Projects\\\\Python\\\\ELC\\\\VGG16_New.keras\")\n",
    "model_vgg19 = load_model(\"C:\\\\Projects\\\\Python\\\\ELC\\\\VGG19_New.keras\")\n",
    "model_DenseNet = load_model(\"C:\\\\Projects\\\\Python\\\\ELC\\\\DenseNet121_New.keras\")\n",
    "\n",
    "# Define labels for facial expression detection\n",
    "labels = ['closed eyes', 'drowsy', 'half-closed', 'neutral', 'yawning']\n",
    "\n",
    "# Function to preprocess frames for VGG models\n",
    "def preprocess_frame(frame, target_size=(224, 224)):\n",
    "    frame = cv2.resize(frame, target_size)\n",
    "    frame = frame.astype('float32') / 255.0\n",
    "    frame = np.expand_dims(frame, axis=0)\n",
    "    return frame\n",
    "\n",
    "# Function to perform ensemble prediction\n",
    "def ensemble_predictions(preds_vgg16, preds_vgg19, preds_DenseNet):\n",
    "    preds = np.array([preds_vgg16, preds_vgg19, preds_DenseNet])\n",
    "    summed_preds = np.sum(preds, axis=0)\n",
    "    averaged_preds = summed_preds / 3\n",
    "    return averaged_preds, np.argmax(averaged_preds)\n",
    "\n",
    "# Function to calculate Eye Aspect Ratio (EAR)\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = np.linalg.norm(eye[1] - eye[5])\n",
    "    B = np.linalg.norm(eye[2] - eye[4])\n",
    "    C = np.linalg.norm(eye[0] - eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Function to calculate Mouth Aspect Ratio (MAR)\n",
    "def mouth_aspect_ratio(mouth):\n",
    "    A = np.linalg.norm(mouth[2] - mouth[10])  # upper_lip_top - lower_lip_bottom\n",
    "    B = np.linalg.norm(mouth[4] - mouth[8])   # upper_lip_bottom - lower_lip_top\n",
    "    C = np.linalg.norm(mouth[0] - mouth[6])   # left_corner - right_corner\n",
    "    mar = (A + B) / (2.0 * C)\n",
    "    return mar\n",
    "\n",
    "# Function to calculate head tilt using nose landmarks\n",
    "def calculate_head_tilt(nose_tip, chin):\n",
    "    delta_x = chin[0] - nose_tip[0]\n",
    "    delta_y = chin[1] - nose_tip[1]\n",
    "    angle = np.arctan2(delta_y, delta_x) * 180 / np.pi\n",
    "    return angle\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default webcam\n",
    "\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open webcam.\")\n",
    "    exit()\n",
    "\n",
    "# Frame rate (frames per second)\n",
    "frame_rate = 30\n",
    "\n",
    "# Calculate the interval between frames in milliseconds\n",
    "frame_interval = int(1000 / frame_rate)\n",
    "# Threshold for mouth aspect ratio to determine if mouth is open\n",
    "mouth_open_threshold = 0.17  # Adjust as needed\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Error: Failed to capture image\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces using Haar Cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255,255, 255), 2)\n",
    "        \n",
    "        # Detect landmarks using face_recognition\n",
    "        face_landmarks_list = face_recognition.face_landmarks(face)\n",
    "        if not face_landmarks_list:\n",
    "            continue\n",
    "        \n",
    "        face_landmarks = face_landmarks_list[0]\n",
    "        \n",
    "        # Get the landmarks for the eyes, mouth, nose, and chin\n",
    "        left_eye = np.array(face_landmarks['left_eye'])\n",
    "        right_eye = np.array(face_landmarks['right_eye'])\n",
    "        top_lip = np.array(face_landmarks['top_lip'])\n",
    "        bottom_lip = np.array(face_landmarks['bottom_lip'])\n",
    "        nose_tip = np.array(face_landmarks['nose_tip'][2])\n",
    "        chin = np.array(face_landmarks['chin'][8])\n",
    "        mouth = np.concatenate((top_lip, bottom_lip), axis=0)\n",
    "        \n",
    "        # Calculate EAR\n",
    "        ear_left = eye_aspect_ratio(left_eye)\n",
    "        ear_right = eye_aspect_ratio(right_eye)\n",
    "        \n",
    "        # Calculate MAR\n",
    "        mar = mouth_aspect_ratio(mouth)\n",
    "        \n",
    "        # Calculate head tilt\n",
    "        head_tilt_angle = calculate_head_tilt(nose_tip, chin)\n",
    "        \n",
    "        # Determine the label based on EAR, MAR, and head tilt\n",
    "        if mar > mouth_open_threshold:\n",
    "            label = 'yawning'\n",
    "        elif ear_left > 0.23 and ear_right > 0.23:\n",
    "            label = 'neutral'\n",
    "        elif ear_left < 0.20 and ear_right < 0.20 and head_tilt_angle > 15:  # Adjust the threshold for head tilt as needed\n",
    "            label = 'drowsy'            \n",
    "        elif 0.20 <= ear_left < 0.23 or 0.2 <= ear_right < 0.23:\n",
    "            label = 'half-closed'\n",
    "        else:\n",
    "            face_preprocessed = preprocess_frame(face)\n",
    "            \n",
    "            # Predict the label using ensembled models\n",
    "            preds_vgg16 = model_vgg16.predict(face_preprocessed)\n",
    "            preds_vgg19 = model_vgg19.predict(face_preprocessed)\n",
    "            preds_DenseNet = model_DenseNet.predict(face_preprocessed)\n",
    "            \n",
    "            averaged_preds, final_prediction = ensemble_predictions(preds_vgg16, preds_vgg19, preds_DenseNet)\n",
    "            \n",
    "            # Check if final_prediction index is within valid range\n",
    "            if final_prediction < len(labels):\n",
    "                label = labels[final_prediction]\n",
    "            else:\n",
    "                label = \"Unknown\"\n",
    "        \n",
    "        # Display the label on the frame\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "        \n",
    "        # Mark points on eyes and mouth\n",
    "        for (ex, ey) in left_eye:\n",
    "            cv2.circle(face, (ex, ey), 2, (144,238,144), -1)\n",
    "        for (ex, ey) in right_eye:\n",
    "            cv2.circle(face, (ex, ey), 2, (144,238,144), -1)\n",
    "        for (ex, ey) in mouth:\n",
    "            cv2.circle(face, (ex, ey), 2, (144,238,144), -1)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(frame_interval) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7bb22b-0330-4e61-881e-58f87af5a20f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
